--
title: "Untitled"
author: "vidhu"
date: "June 15, 2018"
output: html_document
---

```{r}

levels(Adult_dummy_del$income) =      make.names(levels(factor(Adult_dummy_del$income)))

```
# Control for smote modelling
```{r}
set.seed(400)
ctrl = trainControl(method = "cv", 
                     number = 5, 
                     sampling = "smote",
                     summaryFunction= twoClassSummary, 
                     classProbs=T,
                     savePredictions = T)
```
# kNN -smote
```{r}
grid = expand.grid(k=c(19,31,41,51,101, 125,151, 201))
knnFit = train(Adult_dummy_del[,P1], Adult_dummy_del[,"income"], method = "knn", use.all = T,trControl = ctrl, metric = "ROC", tuneGrid = grid)
plot(knnFit)
```


# Random Forest- smote
```{r}
rfFit = train(Adult_dummy_del[,P1], Adult_dummy_del[,"income"], method = "rf", trControl = ctrl, metric = "ROC", tuneLength = 5)
plot(rfFit)
```

```{r}
grid = expand.grid(.mtry =2)
rfFit_tuned = train(Adult_dummy_del[,P1], Adult_dummy_del[,"income"], method = "rf", trControl = ctrl, metric = "ROC",ntree =100, tuneGrid = grid)

```

Result of above chunk

Random Forest 

45175 samples
   20 predictor
    2 classes: 'X0', 'X1' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 36140, 36140, 36140, 36140, 36140 
Addtional sampling using SMOTE

Resampling results:

  ROC        Sens       Spec    
  0.8819329  0.7902161  0.806107

Tuning parameter 'mtry' was held constant at a value of 2
--------
Even ran the rf for ntree = 300 and 1000. 
for 300, the roc was lower and for 1000, it was not able to fit the model. So the tunned model will have ntree = 500 and mtry = 2
--------
# Support vector Machine model
```{r}

svm_linearFit = train(Adult_dummy_del[,P1], Adult_dummy_del[,"income"], method = "svmLinear", trControl = ctrl, metric = "ROC", tuneLength = 20)

```


```{r}
grid <- expand.grid(C = c(0.05, 0.1, 0.25, 0.5, 0.75, 1, 1.25, 1.5, 1.75, 2,5))
svm_linearFit_tuned = train(Adult_dummy_del[,P1], Adult_dummy_del[,"income"], method = "svmLinear", trControl = ctrl, metric = "ROC", tuneGrid = grid)

```

```{r}

svm_RadialFit = train(Adult_dummy_del[,P1], Adult_dummy_del[,"income"], method = "svmRadial", trControl = ctrl, metric = "ROC", tuneLength = 10)

```



# Gradient boosting Smote

```{r}

gbmFit = train(Adult_dummy_del[,P1], Adult_dummy_del[,"income"], method = "gbm", trControl = ctrl, metric = "ROC", tuneLength = 10)

```

#Logistic Regression - smote
```{r}
glmFit = train(Adult_dummy_del[,P1], Adult_dummy_del[,"income"], method = "glm", trControl = ctrl, metric = "ROC", family="binomial")

```


```{r}
resamps= resamples(list(knn = knnFit, rf = rfFit, svm = svm_linearFit_tuned, gbm = gbmFit, glm = glmFit))
summary(resamps)
theme1 <- trellis.par.get()
theme1$plot.symbol$col = rgb(.2, .2, .2, .4)
theme1$plot.symbol$pch = 16
theme1$plot.line$col = rgb(1, 0, 0, .7)
theme1$plot.line$lwd <- 2
trellis.par.set(theme1)
bwplot(resamps, layout = c(3, 1))
difValues <- diff(resamps)
summary(difValues)
parallelplot(resamps)
trellis.par.set(caretTheme())
dotplot(resamps, metric = "ROC")

```




NoW all models with all 37 features
```{r}
grid = expand.grid(k=c(19,31,41,51,101, 125,151, 201))
KNNFit = train(Adult_dummy_del[,results$optVariables], Adult_dummy_del[,"income"], method = "knn", use.all = T,trControl = ctrl, metric = "ROC", tuneGrid = grid)
```

```{r}
RFFit = train(Adult_dummy_del[,results$optVariables], Adult_dummy_del[,"income"], method = "rf", trControl = ctrl, metric = "ROC", tuneLength = 5)

```

```{r}
grid <- expand.grid(C = c(0.05, 0.1, 0.25, 0.5, 0.75, 1, 1.5, 2,5))
SVM_linearFit = train(Adult_dummy_del[,results$optVariables], Adult_dummy_del[,"income"], method = "svmLinear", trControl = ctrl, metric = "ROC", tuneGrid = grid)


```


```{r}

GBMFit = train(Adult_dummy_del[,results$optVariables], Adult_dummy_del[,"income"], method = "gbm", trControl = ctrl, metric = "ROC", tuneLength = 10)

```


```{r}
GLMFit = train(Adult_dummy_del[,results$optVariables], Adult_dummy_del[,"income"], method = "glm", trControl = ctrl, metric = "ROC", family="binomial")
#comparison with glmFit
anova(GLMFit$finalModel, test = "LRT")
#some variables are  less significant.
```


After removing less significant variables.
```{r}
lessPvalues = results$optVariables[c(1:24,26:28,30:31,33:37)]
GLM1Fit = train(Adult_dummy_del[,lessPvalues], Adult_dummy_del[,"income"], method = "glm", trControl = ctrl, metric = "ROC", family="binomial")
# Now all variables are significant. we will compare this model with the glmFit.
anova(GLMFit$finalModel,GLM1Fit$finalModel, glmFit$finalModel,test = "Chisq")
```

Neural Network Model
```{r}
my.grid = expand.grid(.decay = c(0,0.5, 0.1), .size = c(1,3,5, 6, 7))
  
nnetFit= train(Adult_dummy_del[,results$optVariables], Adult_dummy_del[,"income"], method='nnet', trControl = ctrl, metric = "ROC", tuneGrid = my.grid, maxit= 200)
```

# Comparing models
```{r}
Resamps= resamples(list(KNN=KNNFit, RF= RFFit, SVM = SVM_linearFit,GBM = GBMFit, GLM =GLMFit, NNet= nnetFit))
summary(Resamps)
theme1 <- trellis.par.get()
theme1$plot.symbol$col = rgb(.2, .2, .2, .4)
theme1$plot.symbol$pch = 16
theme1$plot.line$col = rgb(1, 0, 0, .7)
theme1$plot.line$lwd <- 2
trellis.par.set(theme1)
bwplot(Resamps, layout = c(3, 1))
DifValues <- diff(Resamps)
summary(DifValues)
parallelplot(Resamps)
trellis.par.set(caretTheme())
dotplot(Resamps, metric = "ROC")

```


# Run knn and gbm with imputing missing values to see improvement in the models.
```{r}
Adult_dummy_imputed = knnImputation(Adult_dummy)
colnames(Adult_dummy_imputed)[colnames(Adult_dummy_imputed)=="income..50K"] = "income"
levels(Adult_dummy_imputed$income) =      make.names(levels(factor(Adult_dummy_imputed$income)))
```

```{r}
KNNFit_imputed = train(Adult_dummy_imputed[,results$optVariables], Adult_dummy_imputed[,"income"], method = "knn", use.all = T,trControl = ctrl, metric = "ROC", tuneGrid = expand.grid(k= 125))
```

```{r}
gbmFit_imputed = train(Adult_dummy_imputed[,results$optVariables], Adult_dummy_imputed[,"income"], method = "gbm",trControl = ctrl, metric = "ROC", tuneGrid = expand.grid(n.trees = 100, interaction.depth = 10 , shrinkage = 0.1, n.minobsinnode = 10))
```

```{r}
GBMFit_imputed = train(Adult_dummy_imputed[,results$optVariables], Adult_dummy_imputed[,"income"], method = "gbm",trControl = ctrl, metric = "ROC", tuneGrid = expand.grid(n.trees = 450, interaction.depth =
 6, shrinkage = 0.1, n.minobsinnode = 10))

```

```{r}
Grid = expand.grid(n.trees = 100, interaction.depth =
 10, shrinkage = 0.1, n.minobsinnode = 10)
gbmFit_imputed = train(Adult_dummy_imputed[,P1], Adult_dummy_imputed[,"income"], method = "gbm", trControl = ctrl, metric = "ROC", tuneGrid = Grid)
```

```{r}
# predictions
temp_knn = knnFit$pred[knnFit$pred$k==101,]
predictions_knn = temp_knn$pred[order(temp_knn$rowIndex)]

temp_rf = rfFit$pred[rfFit$pred$mtry==2,]
predictions_rf = temp_rf$pred[order(temp_rf$rowIndex)]

temp_svm = svm_linearFit_tuned$pred[svm_linearFit_tuned$pred$C==0.25,]
predictions_svm = temp_svm$pred[order(temp_svm$rowIndex)]

temp_gbm = gbmFit$pred[which(gbmFit$pred$interaction.depth==10 & gbmFit$pred$n.trees==100 & gbmFit$pred$shrinkage==0.1& gbmFit$pred$n.minobsinnode == 10),]
predictions_gbm = temp_gbm$pred[order(temp_gbm$rowIndex)]

predictions_glm = glmFit$pred$pred[order(glmFit$pred$rowIndex)]

predictions_GLM = GLMFit$pred$pred[order(GLMFit$pred$rowIndex)]

predictions_GLM1 = GLM1Fit$pred$pred[order(GLM1Fit$pred$rowIndex)]

temp_KNN = KNNFit$pred[KNNFit$pred$k==125,]
predictions_KNN = temp_KNN$pred[order(temp_KNN$rowIndex)]
temp_RF = RFFit$pred[RFFit$pred$mtry==2,]
predictions_RF = temp_RF$pred[order(temp_RF$rowIndex)]

temp_GBM = GBMFit$pred[which(GBMFit$pred$interaction.depth==6 & GBMFit$pred$n.trees==450 & GBMFit$pred$shrinkage==0.1, GBMFit$pred$n.minobsinnode == 10),]
predictions_GBM = temp_GBM$pred[order(temp_GBM$rowIndex)]
temp_SVM = SVM_linearFit$pred[SVM_linearFit$pred$C==0.05,]
prediction_SVM = temp_SVM$pred[order(temp_SVM$rowIndex)]
temp_NNet = nnetFit$pred[nnetFit$pred$size==6&nnetFit$pred$decay==0.5,]
prediction_NNet = temp_NNet$pred[order(temp_NNet$rowIndex)]

#Confusion Matrix
cm_knn = confusionMatrix(predictions_knn,Adult_dummy_del[,"income"])
cm_rf = confusionMatrix(predictions_rf,Adult_dummy_del[,"income"])
cm_gbm = confusionMatrix(predictions_gbm,Adult_dummy_del[,"income"])
cm_svm = confusionMatrix(predictions_svm,Adult_dummy_del[,"income"])
cm_glm = confusionMatrix(predictions_glm,Adult_dummy_del[,"income"])
cm_GLM = confusionMatrix(predictions_GLM,Adult_dummy_del[,"income"])
cm_GLM1 = confusionMatrix(predictions_GLM1,Adult_dummy_del[,"income"])
cm_KNN = confusionMatrix(predictions_KNN,Adult_dummy_del[,"income"])
cm_GBM = confusionMatrix(predictions_GBM,Adult_dummy_del[,"income"])
cm_SVM = confusionMatrix(prediction_SVM, Adult_dummy_del[,"income"] )
cm_RF = confusionMatrix(predictions_RF, Adult_dummy_del[, "income"])
cm_NNet = confusionMatrix(prediction_NNet,Adult_dummy_del[, "income"] )
```

```{r}
# Plotting ROC
library(pROC)
plot.roc(KNNFit$pred$obs[KNNFit$pred$k == 125],KNNFit$pred$X0[KNNFit$pred$k == 125], col = "blue")

plot.roc(RFFit$pred$obs[RFFit$pred$mtry == 2],RFFit$pred$X0[RFFit$pred$mtry == 2], add= T, col = "black")

plot.roc(GBMFit$pred$obs[GBMFit$pred$interaction.depth==6 & GBMFit$pred$n.trees==450 & GBMFit$pred$shrinkage==0.1& GBMFit$pred$n.minobsinnode == 10], GBMFit$pred$X0[GBMFit$pred$interaction.depth==6 & GBMFit$pred$n.trees==450 & GBMFit$pred$shrinkage==0.1& GBMFit$pred$n.minobsinnode == 10], add = T, col = "red")

plot.roc(SVM_linearFit$pred$obs[SVM_linearFit$pred$C==0.05], SVM_linearFit$pred$X0[SVM_linearFit$pred$C==0.05], add = T,col = "green")
plot.roc(GLMFit$pred$obs, GLMFit$pred$X0, add =T, col = "purple")
plot.roc(nnetFit$pred$obs[nnetFit$pred$size==6&nnetFit$pred$decay==0.5], nnetFit$pred$X0[nnetFit$pred$size==6&nnetFit$pred$decay==0.5], add = T, col = "yellow")

```



```{r}
#Table for comparison
col1=c(cm_knn$byClass["Balanced Accuracy"],cm_rf$byClass["Balanced Accuracy"], cm_svm$byClass["Balanced Accuracy"], cm_gbm$byClass["Balanced Accuracy"], cm_glm$byClass["Balanced Accuracy"])
col2=c(cm_knn$byClass["F1"],cm_rf$byClass["F1"], cm_svm$byClass["F1"], cm_gbm$byClass["F1"], cm_glm$byClass["F1"])
col3=c(cm_knn$overall["Accuracy"],cm_rf$overall["Accuracy"], cm_svm$overall["Accuracy"], cm_gbm$overall["Accuracy"], cm_glm$overall["Accuracy"])
table = data.frame(BalancedAccuracy = col1, Fmeasure = col2, Accuracy = col3)
rownames(table) = c("kNN", "Random Forests", "Support vector Machine", "Gradient Boost", "Regression")

#Table for comparison
col1=c(cm_KNN$byClass["Balanced Accuracy"],cm_RF$byClass["Balanced Accuracy"], cm_SVM$byClass["Balanced Accuracy"] , cm_GBM$byClass["Balanced Accuracy"], cm_GLM$byClass["Balanced Accuracy"], cm_NNet$byClass["Balanced Accuracy"])
col2=c(cm_KNN$byClass["F1"],cm_RF$byClass["F1"], cm_SVM$byClass["F1"], cm_GBM$byClass["F1"], cm_GLM$byClass["F1"], cm_NNet$byClass["F1"])
col3=c(cm_KNN$overall["Accuracy"],cm_RF$overall["Accuracy"] , cm_SVM$overall["Accuracy"], cm_GBM$overall["Accuracy"], cm_GLM$overall["Accuracy"], cm_NNet$overall["Accuracy"])
Table = data.frame(BalancedAccuracy = col1, Fmeasure = col2, Accuracy = col3)
rownames(Table) = c("kNN", "Random Forests", "Support Vector Machine","Gradient Boost","Generalized Linear Model", "Neural Network")
Table
```



